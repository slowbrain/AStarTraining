{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JULIA MPI First Example: pi computaton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step was to load MPI on my mac.  Seems mpich and openmpi are two reasonable choices\n",
    "with probably no beginner's reason to prefer one over the other. <br>\n",
    "\n",
    "I did  <t> brew install gcc </t> first to get the gcc compiler.  I ran into problems.  \n",
    "The magic thing that told me what to do was <t> brew doctor </t>.  It wanted me to type\n",
    "<t> xcode-select --install </t> and when I did, all was good.  I then typed\n",
    "<t> brew install mpich </t> and mpi was just working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first example was to reproduce <a href=\"http://www.mcs.anl.gov/research/projects/mpi/tutorial/mpiexmpl/src/pi/C/solution.html\">\n",
    "the classic mypi </a> in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Cloning cache of MPI from git://github.com/JuliaParallel/MPI.jl.git\n",
      "INFO: Installing MPI v0.5.0\n",
      "WARNING: OhMyREPL not installed\n",
      "INFO: Building MPI\n",
      "INFO: Recompiling stale cache file /Users/dpsanders/.julia/lib/v0.4/BinDeps.ji for module BinDeps.\n",
      "INFO: Attempting to Create directory /Users/dpsanders/.julia/v0.4/MPI/deps/build\n",
      "INFO: Changing Directory to /Users/dpsanders/.julia/v0.4/MPI/deps/build\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The Fortran compiler identification is GNU 5.3.0\n",
      "-- The C compiler identification is AppleClang 8.0.0.8000042\n",
      "-- Checking whether Fortran compiler has -isysroot\n",
      "-- Checking whether Fortran compiler has -isysroot - yes\n",
      "-- Checking whether Fortran compiler supports OSX deployment target flag\n",
      "-- Checking whether Fortran compiler supports OSX deployment target flag - yes\n",
      "-- Check for working Fortran compiler: /usr/local/bin/gfortran\n",
      "-- Check for working Fortran compiler: /usr/local/bin/gfortran  -- works\n",
      "-- Detecting Fortran compiler ABI info\n",
      "-- Detecting Fortran compiler ABI info - done\n",
      "-- Checking whether /usr/local/bin/gfortran supports Fortran 90\n",
      "-- Checking whether /usr/local/bin/gfortran supports Fortran 90 -- yes\n",
      "-- Check for working C compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc\n",
      "-- Check for working C compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc -- works\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Found Git: /Applications/Julia-0.4.6.app/Contents/Resources/julia/libexec/git-core/git (found version \"1.8.5.6\") \n",
      "-- Configuring incomplete, errors occurred!\n",
      "See also \"/Users/dpsanders/.julia/v0.4/MPI/deps/build/CMakeFiles/CMakeOutput.log\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CMake Error at /usr/local/Cellar/cmake/3.4.3/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:148 (message):\n",
      "  Could NOT find MPI_C (missing: MPI_C_LIBRARIES MPI_C_INCLUDE_PATH)\n",
      "Call Stack (most recent call first):\n",
      "  /usr/local/Cellar/cmake/3.4.3/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:388 (_FPHSA_FAILURE_MESSAGE)\n",
      "  /usr/local/Cellar/cmake/3.4.3/share/cmake/Modules/FindMPI.cmake:614 (find_package_handle_standard_args)\n",
      "  CMakeLists.txt:5 (find_package)\n",
      "\n",
      "\n",
      "=================================[ ERROR: MPI ]=================================\n",
      "\n",
      "LoadError: failed process: Process(`cmake -DCMAKE_INSTALL_PREFIX=/Users/dpsanders/.julia/v0.4/MPI/deps/src -DCMAKE_LIB_INSTALL_PREFIX=/Users/dpsanders/.julia/v0.4/MPI/deps/usr/lib ..`, ProcessExited(1)) [1]\n",
      "while loading /Users/dpsanders/.julia/v0.4/MPI/deps/build.jl, in expression starting on line 54\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================[ BUILD ERRORS ]================================\n",
      "\n",
      "WARNING: MPI had build errors.\n",
      "\n",
      " - packages with build errors remain installed in /Users/dpsanders/.julia/v0.4\n",
      " - build the package(s) and all dependencies with `Pkg.build(\"MPI\")`\n",
      " - build a single package by running its `deps/build.jl` script\n",
      "\n",
      "================================================================================\n",
      "INFO: Package database updated\n",
      "INFO: METADATA is out-of-date â€” you may not have the latest version of MPI\n",
      "INFO: Use `Pkg.update()` to get the latest versions of your packages\n"
     ]
    }
   ],
   "source": [
    "Pkg.add(\"MPI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Precompiling module MPI...\n",
      "ERROR: LoadError: MPI not properly installed. Please run Pkg.build(\"MPI\")\n",
      " in error at /Applications/Julia-0.4.6.app/Contents/Resources/julia/lib/julia/sys.dylib\n",
      " in include at /Applications/Julia-0.4.6.app/Contents/Resources/julia/lib/julia/sys.dylib\n",
      " in include_from_node1 at /Applications/Julia-0.4.6.app/Contents/Resources/julia/lib/julia/sys.dylib\n",
      " [inlined code] from none:2\n",
      " in anonymous at no file:0\n",
      " in process_options at /Applications/Julia-0.4.6.app/Contents/Resources/julia/lib/julia/sys.dylib\n",
      " in _start at /Applications/Julia-0.4.6.app/Contents/Resources/julia/lib/julia/sys.dylib\n",
      "while loading /Users/dpsanders/.julia/v0.4/MPI/src/MPI.jl, in expression starting on line 13\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "Failed to precompile MPI to /Users/dpsanders/.julia/lib/v0.4/MPI.ji",
     "output_type": "error",
     "traceback": [
      "Failed to precompile MPI to /Users/dpsanders/.julia/lib/v0.4/MPI.ji",
      "",
      " in error at /Applications/Julia-0.4.6.app/Contents/Resources/julia/lib/julia/sys.dylib",
      " in compilecache at loading.jl:400",
      " in require at /Applications/Julia-0.4.6.app/Contents/Resources/julia/lib/julia/sys.dylib"
     ]
    }
   ],
   "source": [
    "using MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: MPIManager not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: MPIManager not defined",
      ""
     ]
    }
   ],
   "source": [
    "m = MPIManager(np=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Array{Int64,1}:\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addprocs(m)\n",
    "#@mpi_do m comm = MPI.COMM_WORLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@mpi_do m comm = MPI.COMM_WORLD\n",
    "#\n",
    "# Enter number of intervals, and tell every processor\n",
    "# Traditional MPI would do this with a BCAST\n",
    "@mpi_do m n=45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 3:\t45\n",
      "\tFrom worker 7:\t45\n",
      "\tFrom worker 4:\t45\n",
      "\tFrom worker 8:\t45\n",
      "\tFrom worker 2:\t45\n",
      "\tFrom worker 9:\t45\n",
      "\tFrom worker 6:\t45\n",
      "\tFrom worker 5:\t45\n"
     ]
    }
   ],
   "source": [
    "# Let's see if the processors got it\n",
    "@mpi_do m println(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 4:\t2\n",
      "\tFrom worker 7:\t5\n",
      "\tFrom worker 2:\t0\n",
      "\tFrom worker 9:\t7\n",
      "\tFrom worker 3:\t1\n",
      "\tFrom worker 8:\t6\n",
      "\tFrom worker 5:\t3\n",
      "\tFrom worker 6:\t4\n"
     ]
    }
   ],
   "source": [
    "# my MPI id\n",
    "@mpi_do m myid = MPI.Comm_rank(comm)\n",
    "@mpi_do m println(myid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 7:\t8\n",
      "\tFrom worker 2:\t8\n",
      "\tFrom worker 9:\t8\n",
      "\tFrom worker 4:\t8\n",
      "\tFrom worker 3:\t8\n",
      "\tFrom worker 5:\t8\n",
      "\tFrom worker 8:\t8\n",
      "\tFrom worker 6:\t8\n"
     ]
    }
   ],
   "source": [
    "# Get the number of processors\n",
    "@mpi_do m np=MPI.Comm_size(comm)\n",
    "@mpi_do m println(np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $\\int_0^1 4/(1+x^2) dx= 4 atan(x)]_0^1$ which evaluates to $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 2:\t3.1415926535897043\n",
      "\tFrom worker 2:\t-8.881784197001252e-14\n",
      "  3.965622 seconds (32.83 k allocations: 1.540 MB)\n"
     ]
    }
   ],
   "source": [
    "@time @mpi_do m (\n",
    "   n = 50_000_000;\n",
    "   comm = MPI.COMM_WORLD;\n",
    "   s=0.0;\n",
    "   for i= MPI.Comm_rank(comm)+1 :  MPI.Comm_size(comm) : n \n",
    "    x = (i-.5)/n \n",
    "    s += 4/(1+x^2) \n",
    "   end;\n",
    "   mypi = s/n;\n",
    "   our_Ï€ = MPI.Reduce(mypi, MPI.SUM, 0, comm);\n",
    "   if myid==0\n",
    "      println(our_Ï€);\n",
    "      println(our_Ï€ - Ï€); \n",
    "   end\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11.396405 seconds (350.06 M allocations: 5.963 GB, 4.10% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time (n = 50_000_000;\n",
    "h=1/n;\n",
    " our_Ï€ =0;\n",
    "for i=0:h:1\n",
    "    our_Ï€ += 4/(1+i^2)\n",
    "end;\n",
    "#our_Ï€*h\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "3.7*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.4.6",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.6"
  },
  "toc": {
   "nav_menu": {
    "height": "29px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "2",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
